{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e6b882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3730f",
   "metadata": {},
   "source": [
    "### 2. ConfiguraciÃ³n e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922e7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ”§ 2. ConfiguraciÃ³n e Importaciones\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.tools import FunctionTool\n",
    "#from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, Filter, FieldCondition, Range\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b13d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Obtener la API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"âŒ No se encontrÃ³ OPENAI_API_KEY en el .env\")\n",
    "\n",
    "# ConfiguraciÃ³n Global\n",
    "client = OpenAI(api_key=OPENAI_API_KEY) # Cliente nativo para embeddings manuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a31aad",
   "metadata": {},
   "source": [
    "#### Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3c74e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='bio_semantic_db')]\n",
      "âœ… ConfiguraciÃ³n cargada.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Obtener valores\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# Validar variables\n",
    "if not QDRANT_URL:\n",
    "    raise ValueError(\"âŒ No se encontrÃ³ QDRANT_URL en el .env\")\n",
    "\n",
    "if not QDRANT_API_KEY:\n",
    "    raise ValueError(\"âŒ No se encontrÃ³ QDRANT_API_KEY en el .env\")\n",
    "\n",
    "qdrant_client = QdrantClient(\n",
    "    url= QDRANT_URL, \n",
    "    api_key= QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "print(qdrant_client.get_collections())\n",
    "\n",
    "COLLECTION_NAME = \"metabolomics_agent_db\"\n",
    "\n",
    "print(\"âœ… ConfiguraciÃ³n cargada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc46095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7d294ca",
   "metadata": {},
   "source": [
    "### 3. Ingesta con Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c7b459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Leyendo PDFs...\n",
      "ðŸ§  Analizando significado y creando chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:11:47,634 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:49,390 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:50,914 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:54,125 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:55,281 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:57,515 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:58,232 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:11:59,575 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:01,690 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:03,533 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Se crearon 37 chunks semÃ¡nticos.\n",
      "ðŸš€ Subiendo a Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 16:12:08,788 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:10,454 - INFO - HTTP Request: PUT https://f71299c2-b428-4945-b4c4-eadd84c589da.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/bio_semantic_db \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:10,972 - INFO - HTTP Request: PUT https://f71299c2-b428-4945-b4c4-eadd84c589da.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/bio_semantic_db/index?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:11,216 - INFO - HTTP Request: GET https://f71299c2-b428-4945-b4c4-eadd84c589da.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/bio_semantic_db \"HTTP/1.1 200 OK\"\n",
      "2025-11-24 16:12:13,375 - INFO - HTTP Request: PUT https://f71299c2-b428-4945-b4c4-eadd84c589da.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/bio_semantic_db/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Â¡Ingesta Lista!\n"
     ]
    }
   ],
   "source": [
    "# @title ðŸ§  3. Ingesta con Semantic Chunking (La parte clave)\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === NUEVO: detectar raÃ­z del proyecto y crear DATA ahÃ­ ===\n",
    "CURRENT_DIR = Path().resolve()\n",
    "PROJECT_ROOT = CURRENT_DIR.parent       # sube dos niveles desde notebooks/\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# 1. Crear carpeta y pedir PDFs\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âš ï¸ CARPETA 'data' CREADA EN: {DATA_DIR}. Por favor, sube tus PDFs ahÃ­ antes de continuar.\")\n",
    "else:\n",
    "    # 2. Configurar Modelos\n",
    "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.chunk_size = 1024\n",
    "\n",
    "    # 3. Configurar Qdrant como almacÃ©n\n",
    "    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=\"bio_semantic_db\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # 4. DEFINIR EL SPLITTER SEMÃNTICO (AquÃ­ estÃ¡ la magia)\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "        buffer_size=1,\n",
    "        breakpoint_percentile_threshold=95,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    # 5. Cargar y Procesar\n",
    "    print(\"ðŸ“‚ Leyendo PDFs...\")\n",
    "    docs = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "    \n",
    "    print(\"ðŸ§  Analizando significado y creando chunks...\")\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "    print(f\"   -> Se crearon {len(nodes)} chunks semÃ¡nticos.\")\n",
    "\n",
    "    # 6. Indexar\n",
    "    print(\"ðŸš€ Subiendo a Qdrant...\")\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "    print(\"âœ… Â¡Ingesta Lista!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de2fbc",
   "metadata": {},
   "source": [
    "### Agente ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc0cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae48be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model= model,\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a helpful research assistant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a23b237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 01:45:28,097 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hola!\"}\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd6b30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hola!', additional_kwargs={}, response_metadata={}, id='8a072b79-5581-4369-a4b5-440eb2559ada'),\n",
       "  AIMessage(content='Â¡Hola! Â¿CÃ³mo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 20, 'total_tokens': 29, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CfKcAieag9RgQCdhKMOO3PXgOJIZU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--45fad6b4-6796-422e-9204-b96cb7e84d39-0', usage_metadata={'input_tokens': 20, 'output_tokens': 9, 'total_tokens': 29, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
